{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyTorch Deployment & Optimization Playbook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Senior MLEs often need repeatable recipes for squeezing the most out of PyTorch models.\n",
        "This notebook highlights practical techniques with bite-sized examples that you can copy into\n",
        "production scripts. We work with a lightweight classifier so you can focus on the tooling, not the math.\n",
        "\n",
        "**Topics covered**\n",
        "- Latency and inference speed optimizations\n",
        "- Memory efficiency patterns (training & inference)\n",
        "- Quantization for CPU deployments\n",
        "- Structured pruning to shrink models\n",
        "- Exporting models to ONNX, TensorRT, and TFLite toolchains\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import time\n",
        "from typing import Iterable\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)\n",
        "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline model and synthetic data\n",
        "Keep the network intentionally small - this makes it easy to experiment with tooling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98e1885d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TinyClassifier(nn.Module):\n",
        "    def __init__(self, in_features: int = 128, hidden: Iterable[int] = (256, 128), num_classes: int = 10, dropout_p: float = 0.1):\n",
        "        super().__init__()\n",
        "        hidden = tuple(hidden)\n",
        "        layers = []\n",
        "        last = in_features\n",
        "        for width in hidden:\n",
        "            layers.append(nn.Linear(last, width))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout_p))\n",
        "            last = width\n",
        "        layers.append(nn.Linear(last, num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "model = TinyClassifier().to(device)\n",
        "dummy_batch = torch.randn(512, 128, device=device)\n",
        "dummy_targets = torch.randint(0, 10, (512,), device=device)\n",
        "\n",
        "print(model(dummy_batch).shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff368257",
      "metadata": {},
      "source": [
        "## Latency & inference speed\n",
        "A reliable way to evaluate optimizations is to benchmark short runs.\n",
        "The helper below keeps timing simple and works on both CPU and GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5372d1ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def benchmark(model: nn.Module, inputs: torch.Tensor, warmup: int = 10, steps: int = 50) -> float:\n",
        "    model.eval()\n",
        "    for _ in range(warmup):\n",
        "        model(inputs)\n",
        "    start = time.perf_counter()\n",
        "    for _ in range(steps):\n",
        "        model(inputs)\n",
        "    end = time.perf_counter()\n",
        "    return (end - start) / steps\n",
        "\n",
        "\n",
        "base_latency = benchmark(model, dummy_batch)\n",
        "print(f\"Baseline latency: {base_latency * 1e3:.3f} ms per batch\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec6f98d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TorchScript for stable inference graphs\n",
        "scripted = torch.jit.script(model)\n",
        "script_latency = benchmark(scripted, dummy_batch)\n",
        "print(f\"Scripted latency: {script_latency * 1e3:.3f} ms per batch\")\n",
        "\n",
        "compiled_latency = None\n",
        "if hasattr(torch, \"compile\"):\n",
        "    compiled = torch.compile(model, mode=\"reduce-overhead\")\n",
        "    compiled_latency = benchmark(compiled, dummy_batch)\n",
        "    print(f\"torch.compile latency: {compiled_latency * 1e3:.3f} ms per batch\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b63b9bd",
      "metadata": {},
      "source": [
        "## Finding Bottlenecks with PyTorch Profiler\n",
        "Before optimizing, you need to know where the time is spent. The PyTorch Profiler is the standard tool for this.\n",
        "It traces operators on both CPU and GPU, helping you identify which parts of your model are the slowest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c94617d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.profiler\n",
        "\n",
        "with torch.profiler.profile(\n",
        "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "    record_shapes=True,\n",
        "    with_stack=True,\n",
        ") as prof:\n",
        "    with torch.profiler.record_function(\"model_inference\"):\n",
        "        for _ in range(10):\n",
        "            model(dummy_batch)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2020c51",
      "metadata": {},
      "source": [
        "**Quick wins for latency**\n",
        "- Switch to `torch.inference_mode()` to bypass autograd entirely during inference.\n",
        "- Use TorchScript (`torch.jit.script`) for backend-agnostic graph compilation.\n",
        "- On PyTorch 2.x, `torch.compile(model, mode=\"reduce-overhead\")` can remove Python dispatch.\n",
        "- When serving on GPU, enable `torch.backends.cudnn.benchmark = True` for convolutional workloads with static shapes.\n",
        "- Pin CPU threads via `torch.set_num_threads()` and align with your serving process affinity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02950ea9",
      "metadata": {},
      "source": [
        "## Memory optimization patterns\n",
        "Even simple models benefit from disciplined memory management, especially when batching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82fc9cc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_step(model: nn.Module, inputs: torch.Tensor, targets: torch.Tensor, *, mixed_precision: bool = False) -> float:\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=mixed_precision and device.type == \"cuda\")\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    with torch.cuda.amp.autocast(enabled=mixed_precision and device.type == \"cuda\"):\n",
        "        logits = model(inputs)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    return float(loss.item())\n",
        "\n",
        "\n",
        "loss = train_step(model, dummy_batch, dummy_targets)\n",
        "print(f\"Single step loss: {loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a362d5e",
      "metadata": {},
      "source": [
        "**Memory checklist**\n",
        "- Use `set_to_none=True` when zeroing gradients to skip redundant memory writes.\n",
        "- Switch on mixed precision (`torch.cuda.amp`) during training to reduce activation footprint on CUDA.\n",
        "- Gradient checkpointing (`torch.utils.checkpoint`) trades compute for memory on deep networks.\n",
        "- Wrap inference in `torch.inference_mode()` to free autograd buffers immediately.\n",
        "- Keep tensor dtypes explicit (for example `torch.float16`, `torch.bfloat16`) for compatibility with quantized or accelerated kernels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7274dda",
      "metadata": {},
      "source": [
        "## Regularization and Training Loop Example\n",
        "Regularization techniques are essential for training robust models that generalize well. The following runnable example demonstrates a complete training loop that incorporates several key concepts:\n",
        "\n",
        "- **Weight Decay (L2 Regularization)**: Applied via the `weight_decay` parameter in the `AdamW` optimizer to penalize large weights.\n",
        "- **Dropout**: Included in our `TinyClassifier` to prevent neuron co-adaptation.\n",
        "- **Gradient Accumulation**: Simulates a larger batch size by accumulating gradients over several steps, crucial for memory-constrained training.\n",
        "- **Early Stopping**: Monitors validation loss and stops training when it no longer improves, saving compute and preventing overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cff76ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 1. Create dummy datasets and dataloaders\n",
        "train_data = TensorDataset(dummy_batch, dummy_targets)\n",
        "val_data = TensorDataset(torch.randn(128, 128, device=device), torch.randint(0, 10, (128,), device=device))\n",
        "train_loader = DataLoader(train_data, batch_size=64)\n",
        "val_loader = DataLoader(val_data, batch_size=128)\n",
        "\n",
        "# 2. Instantiate model and optimizer with Weight Decay\n",
        "training_model = TinyClassifier().to(device)\n",
        "optimizer = torch.optim.AdamW(training_model.parameters(), lr=1e-3, weight_decay=1e-4) # L2 Regularization\n",
        "\n",
        "# 3. Set up training parameters\n",
        "num_epochs = 20\n",
        "accumulation_steps = 4 # Effective batch size = 64 * 4 = 256\n",
        "\n",
        "# 4. Early stopping parameters\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # --- Training Phase ---\n",
        "    training_model.train()\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        logits = training_model(inputs)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "        loss = loss / accumulation_steps # Normalize loss for accumulation\n",
        "        loss.backward()\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    training_model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            logits = training_model(inputs)\n",
        "            val_loss += F.cross_entropy(logits, targets, reduction=\"sum\").item()\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # --- Early Stopping Check ---\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(training_model.state_dict(), \"best_model.pth\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    \n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Stopping early at epoch {epoch+1}. Best val loss: {best_val_loss:.4f}\")\n",
        "        training_model.load_state_dict(torch.load(\"best_model.pth\")) # Restore best model\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic quantization (CPU-centric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model.cpu(),\n",
        "    {nn.Linear},\n",
        "    dtype=torch.qint8,\n",
        ")\n",
        "\n",
        "q_latency = benchmark(quantized_model, dummy_batch.cpu())\n",
        "print(f\"Quantized latency: {q_latency * 1e3:.3f} ms per batch (CPU)\")\n",
        "print(f\"FP32 params: {sum(p.numel() for p in model.cpu().parameters())}\")\n",
        "print(f\"INT8 params: {sum(p.numel() for p in quantized_model.parameters())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Static Quantization (CNN-centric)\n",
        "For models with fixed input sizes like CNNs, static quantization is preferred. It involves a calibration step where we feed representative data through the model to compute activation statistics. This often results in better performance than dynamic quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TinyConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc = nn.Linear(32 * 13 * 13, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "cnn_model = TinyConvNet().eval().cpu()\n",
        "dummy_images = torch.randn(16, 3, 32, 32) # Batch of 16 images\n",
        "\n",
        "# 1. Fuse modules (Conv + ReLU)\n",
        "torch.quantization.fuse_modules(cnn_model, [[\"conv1\", \"relu1\"], [\"conv2\", \"relu2\"]], inplace=True)\n",
        "\n",
        "# 2. Prepare for static quantization\n",
        "cnn_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "cnn_model_prepared = torch.quantization.prepare(cnn_model)\n",
        "\n",
        "# 3. Calibrate with representative data\n",
        "with torch.inference_mode():\n",
        "    for _ in range(10):\n",
        "        cnn_model_prepared(dummy_images)\n",
        "\n",
        "# 4. Convert to a quantized model\n",
        "cnn_quantized = torch.quantization.convert(cnn_model_prepared)\n",
        "\n",
        "fp32_size = sum(p.numel() * p.element_size() for p in cnn_model.parameters())\n",
        "q_size = sum(p.numel() * p.element_size() for p in cnn_quantized.parameters())\n",
        "\n",
        "print(f\"Original CNN size: {fp32_size / 1e3:.1f} KB\")\n",
        "print(f\"Quantized CNN size: {q_size / 1e3:.1f} KB\")\n",
        "\n",
        "q_cnn_latency = benchmark(cnn_quantized, dummy_images)\n",
        "print(f\"Quantized CNN latency: {q_cnn_latency * 1e3:.3f} ms per batch (CPU)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Quantization tips**\n",
        "- Dynamic quantization works out of the box for transformer-style `nn.Linear` layers.\n",
        "- For CNNs, combine static quantization (`prepare_qat` + `convert`) with representative calibration data.\n",
        "- Monitor accuracy drift after quantization. Keep the first or last layer in FP32 if needed.\n",
        "- Quantized models run fastest on CPUs with vector dot product instructions (AVX512-VNNI, ARM dotprod, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structured pruning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn.utils import prune\n",
        "\n",
        "pruned_model = TinyClassifier()\n",
        "parameters_to_prune = [\n",
        "    (module, \"weight\")\n",
        "    for module in pruned_model.modules()\n",
        "    if isinstance(module, nn.Linear)\n",
        "]\n",
        "\n",
        "for module, name in parameters_to_prune:\n",
        "    prune.l1_unstructured(module, name=name, amount=0.3)\n",
        "\n",
        "# To make pruning permanent and remove the reparameterization hooks\n",
        "for module, name in parameters_to_prune:\n",
        "    prune.remove(module, name)\n",
        "\n",
        "nonzero = sum(torch.count_nonzero(m.weight) for m in pruned_model.modules() if isinstance(m, nn.Linear))\n",
        "total = sum(m.weight.numel() for m in pruned_model.modules() if isinstance(m, nn.Linear))\n",
        "print(f\"Remaining weights: {nonzero} / {total} ({nonzero / total:.2%})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pruning workflow**\n",
        "1. Apply magnitude-based pruning with `torch.nn.utils.prune` to identify removable weights.\n",
        "2. Fine-tune the sparse model to recover accuracy.\n",
        "3. Remove pruning reparameterization via `prune.remove(module, \"weight\")` before exporting.\n",
        "4. Export to a format that understands sparsity (for example ONNX with sparsity metadata or TensorRT sparsity).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export to ONNX, TensorRT, and TFLite\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "onnx_path = \"tiny_classifier.onnx\"\n",
        "torch.onnx.export(\n",
        "    model.cpu(),\n",
        "    dummy_batch.cpu(),\n",
        "    onnx_path,\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"logits\"],\n",
        "    dynamic_axes={\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n",
        "    opset_version=17,\n",
        ")\n",
        "print(f\"Saved ONNX graph to {onnx_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conversion playbook**\n",
        "- TensorRT: Optimize the exported ONNX using `trtexec --onnx=tiny_classifier.onnx --saveEngine=model.plan`.\n",
        "  Leverage FP16 or INT8 calibration for mixed precision speedups.\n",
        "- TFLite: Convert ONNX to TensorFlow (for example via `onnx-tf`) and then apply `tf.lite.TFLiteConverter`. For pure PyTorch flows,\n",
        "  `torch.export` followed by FX passes can target TFLite-compatible dialects.\n",
        "- Edge runtimes: Benchmark with `onnxruntime-tools` or `torch_tensorrt` to validate latency versus accuracy budgets.\n",
        "- Always store calibration datasets alongside models so quantization and engine builds are reproducible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next exploration ideas\n",
        "- Profile kernels with PyTorch Profiler (`torch.profiler`) to capture operator-level hotspots.\n",
        "- Combine pruning and quantization (for example sparse INT8) for maximum compression.\n",
        "- Automate artifact builds using CI so every commit produces ONNX or TensorRT packages with latency baselines.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
